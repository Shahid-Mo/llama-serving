FROM ghcr.io/ggml-org/llama.cpp:server

# Install curl
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Download the model during build
RUN mkdir -p /models && \
    curl -L -o /models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
    https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# Command stays as is - base image already has proper ENTRYPOINT
CMD ["-m", "/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf", "--port", "8080", "--host", "0.0.0.0"]